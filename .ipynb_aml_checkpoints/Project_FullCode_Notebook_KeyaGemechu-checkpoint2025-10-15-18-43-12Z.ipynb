{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Business Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An automobile dealership in Los Vegas specializes in selling luxury and non-luxury vehicles. They cater to diverse customer preferences with varying vehicle specifications, such as mileage, engine capacity, and seating capacity. However, the dealership faces significant challenges in maintaining consistency and efficiency across its pricing strategy due to reliance on manual processes and disconnected systems. Pricing evaluations are prone to errors, updates are delayed, and scaling operations are difficult as demand grows. These inefficiencies impact revenue and customer trust. Recognizing the need for a reliable and scalable solution, the dealership is seeking to implement a unified system that ensures seamless integration of data-driven pricing decisions, adaptability to changing market conditions, and operational efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Objective**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dealership has hired you as an MLOps Engineer to design and implement an MLOps pipeline that automates the pricing workflow. This pipeline will encompass data cleaning, preprocessing, transformation, model building, training, evaluation, and registration with CI/CD capabilities to ensure continuous integration and delivery. Your role is to overcome challenges such as integrating disparate data sources, maintaining consistent model performance, and enabling scalable, automated updates to meet evolving business needs. The expected outcomes are a robust, automated system that improves pricing accuracy, operational efficiency, and scalability, driving increased profitability and customer satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Data Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains attributes of used cars sold in various locations. These attributes serve as key data points for CarOnSell's pricing model. The detailed attributes are:\n",
        "\n",
        "- **Segment:** Describes the category of the vehicle, indicating whether it is a luxury or non-luxury segment.\n",
        "\n",
        "- **Kilometers_Driven:** The total number of kilometers the vehicle has been driven.\n",
        "\n",
        "- **Mileage:** The fuel efficiency of the vehicle, measured in kilometers per liter (km/l).\n",
        "\n",
        "- **Engine:** The engine capacity of the vehicle, measured in cubic centimeters (cc). \n",
        "\n",
        "- **Power:** The power of the vehicle's engine, measured in brake horsepower (BHP). \n",
        "\n",
        "- **Seats:** The number of seats in the vehicle, can influence the vehicle's classification, usage, and pricing based on customer needs.\n",
        "\n",
        "- **Price:** The price of the vehicle, listed in lakhs (units of 100,000), represents the cost to the consumer for purchasing the vehicle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. AzureML Environment Setup and Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.1 Connect to Azure Machine Learning Workspace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: fg: no job control\n"
          ]
        }
      ],
      "source": [
        "# Install the Azure Machine Learning SDK and FAISS-related utilities\n",
        "!%pip install azure-ai-ml\n",
        "# %pip install -U 'azureml-rag[faiss,hugging_face]>=0.2.36'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1737786743692
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "from azureml.core import Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing workspace.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile workspace.json\n",
        "{\n",
        "    \"subscription_id\": \"aa382cca-fa09-4ae9-b74b-c63cd0b942e8\",\n",
        "    \"resource_group\":  \"defualt_resource_group\",\n",
        "    \"workspace_name\": \"azureai\"  \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize credentials for Azure authentication\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1737786744030
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: workspace.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x76816173d330>,\n",
            "         subscription_id=aa382cca-fa09-4ae9-b74b-c63cd0b942e8,\n",
            "         resource_group_name=defualt_resource_group,\n",
            "         workspace_name=azureai)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the MLClient to connect with AzureML\n",
        "ml_client = MLClient.from_config(credential=credential, path=\"workspace.json\")\n",
        "\n",
        "\n",
        "\n",
        "# Create an AzureML Workspace object\n",
        "ws = Workspace(\n",
        "    subscription_id=ml_client.subscription_id,\n",
        "    resource_group=ml_client.resource_group_name,\n",
        "    workspace_name=ml_client.workspace_name,\n",
        ")\n",
        "\n",
        "\n",
        "# Verify the client and workspace details\n",
        "print(ml_client)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.2 Set Up Compute Cluster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1737786744266
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You already have a cluster named cpu-cluster, we'll reuse it as is.\n",
            "AMLCompute with name cpu-cluster is created, the compute size is Standard_DS11_v2\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"Standard_DS11_v2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster).result()\n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.3 Register Dataset as Data Asset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1737786744520
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to the local dataset\n",
        "\n",
        "# (\"used_cars.csv\")\n",
        "local_csv = Path (\"/home/azureuser/cloudfiles/code/Users/Keya_1751545169843/car-price-mlops/used_cars.csv\")\n",
        "assert local_csv.exists(), f\"File not found:\"\n",
        "# Path: Users/Keya_1751545169843/car-price-mlops/used_cars.csv\n",
        "# Relative Path: /home/azureuser/cloudfiles/code/Users/Keya_1751545169843/car-price-mlops/used_cars.csv\n",
        "\n",
        "# Set the version number of the data asset (for example: '1')\n",
        "VERSION = \"2\"\n",
        "\n",
        "# Create and register the dataset as an AzureML data asset\n",
        "data_asset = Data(\n",
        "    path=local_csv,\n",
        "    type=AssetTypes.URI_FILE, \n",
        "    description=\"A dataset of used cars for price prediction\",\n",
        "    name=\"used-cars-data\",\n",
        "    version=VERSION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data({'path': 'azureml://subscriptions/aa382cca-fa09-4ae9-b74b-c63cd0b942e8/resourcegroups/defualt_resource_group/workspaces/azureai/datastores/workspaceblobstore/paths/LocalUpload/0b8e06a9f14bf45a52b1c21394f1cdf03017517cd48663b3e20a05882ff35cdd/used_cars.csv', 'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'used-cars-data', 'description': 'A dataset of used cars for price prediction', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/aa382cca-fa09-4ae9-b74b-c63cd0b942e8/resourceGroups/defualt_resource_group/providers/Microsoft.MachineLearningServices/workspaces/azureai/data/used-cars-data/versions/2', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/autoproject/code/Users/Keya_1751545169843/car-price-mlops', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x76816173dd50>, 'serialize': <msrest.serialization.Serializer object at 0x7681265f01f0>, 'version': '2', 'latest_version': None, 'datastore': None})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the data asset in the workspace\n",
        "ml_client.data.create_or_update(data_asset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.4 Create and Configure Job Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1737786747071
        }
      },
      "outputs": [],
      "source": [
        "# Create a directory for the preprocessing script\n",
        "import os\n",
        "\n",
        "src_dir_env = \"./env\"\n",
        "os.makedirs(src_dir_env, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./env/conda.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile {src_dir_env}/conda.yml\n",
        "name: sklearn-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.23.2\n",
        "  - scipy=1.7.1\n",
        "  - pip:  \n",
        "    - mlflow==2.8.1\n",
        "    - azureml-mlflow==1.51.0\n",
        "    - azureml-inference-server-http\n",
        "    - azureml-core==1.49.0\n",
        "    - cloudpickle==1.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1737786749081
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'machine_learning_E2E', 'description': 'Environment created from a Docker image plus Conda environment.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/aa382cca-fa09-4ae9-b74b-c63cd0b942e8/resourceGroups/defualt_resource_group/providers/Microsoft.MachineLearningServices/workspaces/azureai/environments/machine_learning_E2E/versions/1', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/autoproject/code/Users/Keya_1751545169843/car-price-mlops', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x768125c5b160>, 'serialize': <msrest.serialization.Serializer object at 0x768125c5bb80>, 'version': '1', 'conda_file': {'channels': ['conda-forge'], 'dependencies': ['python=3.8', 'pip=21.2.4', 'scikit-learn=0.23.2', 'scipy=1.7.1', {'pip': ['mlflow==2.8.1', 'azureml-mlflow==1.51.0', 'azureml-inference-server-http', 'azureml-core==1.49.0', 'cloudpickle==1.6.0']}], 'name': 'sklearn-env'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.8\",\\n    \"pip=21.2.4\",\\n    \"scikit-learn=0.23.2\",\\n    \"scipy=1.7.1\",\\n    {\\n      \"pip\": [\\n        \"mlflow==2.8.1\",\\n        \"azureml-mlflow==1.51.0\",\\n        \"azureml-inference-server-http\",\\n        \"azureml-core==1.49.0\",\\n        \"cloudpickle==1.6.0\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"sklearn-env\"\\n}'})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
        "    conda_file=\"env/conda.yml\",\n",
        "    name=\"machine_learning_E2E\",\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2. Model Development Workflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2.1 Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This **Data Preparation job** is designed to process an input dataset by splitting it into two parts: one for training the model and the other for testing it. The script accepts three inputs: the location of the input data (`used_cars.csv`), the ratio for splitting the data into training and testing sets (`test_train_ratio`), and the paths to save the resulting training (`train_data`) and testing (`test_data`) data. The script first reads the input CSV data from a data asset URI, then splits it using Scikit-learn's train_test_split function, and saves the two parts to the specified directories. It also logs the number of records in both the training and testing datasets using MLflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "src_dir_job_scripts = \"./data_prep\"\n",
        "os.makedirs(src_dir_job_scripts, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./data_prep/data_prep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {src_dir_job_scripts}/data_prep.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main(): \n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = parser.parse_args()\n",
        "     # Start MLflow Run\n",
        "    mlflow.start_run()\n",
        "\n",
        "    # Log arguments\n",
        "    logging.info(f\"Input data path: {args.data}\")\n",
        "    logging.info(f\"Test-train ratio: {args.test_train_ratio}\")\n",
        "\n",
        "    # Reading Data\n",
        "    df = pd.read_csv(args.data)\n",
        "\n",
        "    # Encode categorical feature\n",
        "    le = LabelEncoder()\n",
        "    df['Segment'] = le.fit_transform(df['Segment'])\n",
        "\n",
        "    # Split Data into train and test datasets\n",
        "    train_df, test_df = train_test_split(df, test_size=args.test_train_ratio, random_state=42)\n",
        "\n",
        "    # Save train and test data\n",
        "    os.makedirs(args.train_data, exist_ok=True)\n",
        "    os.makedirs(args.test_data, exist_ok=True)\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"train.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"test.csv\"), index=False)\n",
        "\n",
        "    # log the metrics\n",
        "    mlflow.log_metric('train size', train_df.shape[0])\n",
        "    mlflow.log_metric('test size', test_df.shape[0])\n",
        "    \n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Define Data Preparation job**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this AzureML job, we define the `command` object that takes input files and output directories, then executes the script with the provided inputs and outputs. The job runs in a pre-configured AzureML environment with the necessary libraries. The result will be two separate datasets for training and testing, ready for use in subsequent steps of the machine learning pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2.2 Training the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This Model Training job is designed to train a **Random Forest Regressor** on the dataset that was split into training and testing sets in the previous data preparation job. This job script accepts five inputs: the path to the training data (`train_data`), the path to the testing data (`test_data`), the number of trees in the forest (`n_estimators`, with a default value of 100), the maximum depth of the trees (`max_depth`, which is set to None by default), and the path to save the trained model (`model_output`).\n",
        "\n",
        "The script begins by reading the training and testing data files, then processes the data to separate features (X) and target labels (y). A Random Forest Regressor model is initialized using the given n_estimators and max_depth, and it is trained using the training data. The model's performance is evaluated using the `Mean Squared Error (MSE)`. The MSE score is logged in MLflow. Finally, the trained model is saved and stored in the specified output location as an MLflow model. The job completes by logging the final MSE score and ending the MLflow run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "src_dir_job_scripts = \"./model_train\"\n",
        "os.makedirs(src_dir_job_scripts, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ./model_train/model_train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {src_dir_job_scripts}/model_train.py\n",
        "\n",
        "# Required imports for training\n",
        "import mlflow\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mlflow.start_run()  # Start the MLflow experiment run\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)  # Create the \"outputs\" directory if it doesn't exist\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects the first file in a folder, assuming there's only one file.\n",
        "    Args:\n",
        "        path (str): Path to the directory or file to choose.\n",
        "    Returns:\n",
        "        str: Full path of the selected file.\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\"train\")\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"Path to train dataset\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"Path to test dataset\")\n",
        "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
        "    parser.add_argument('--n_estimators', type=int, default=100,\n",
        "                        help='The number of trees in the forest')\n",
        "    parser.add_argument('--max_depth', type=int, default=None,\n",
        "                        help='The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load datasets\n",
        "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
        "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Split the data into features(X) and target(y) \n",
        "    y_train = train_df['Price']\n",
        "    X_train = train_df.drop(columns=['Price'])\n",
        "    y_test = test_df['Price']\n",
        "    X_test = test_df.drop(columns=['Price'])\n",
        "\n",
        "    # Initialize and train a RandomForest Regressor\n",
        "    model = RandomForestRegressor(n_estimators=args.n_estimators, max_depth=args.max_depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Log model hyperparameters\n",
        "    mlflow.log_param(\"model\", \"RandomForestRegressor\")\n",
        "    mlflow.log_param(\"n_estimators\", args.n_estimators)\n",
        "    mlflow.log_param(\"max_depth\", args.max_depth)\n",
        "\n",
        "    # Predict using the RandomForest Regressor on test data\n",
        "    yhat_test = model.predict(X_test)\n",
        "\n",
        "    # Compute and log mean squared error for test data\n",
        "    mse = mean_squared_error(y_test, yhat_test)\n",
        "    print('Mean Squared Error of RandomForest Regressor on test set: {:.2f}'.format(mse))\n",
        "    mlflow.log_metric(\"MSE\", float(mse))\n",
        "\n",
        "    # Save the model\n",
        "    mlflow.sklearn.save_model(sk_model=model, path=args.model_output)\n",
        "\n",
        "    mlflow.end_run()  # Ending the MLflow experiment run\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Define Model Training Job**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this AzureML job, we define the `command` object that takes the paths to the training and testing data, the number of trees in the forest (`n_estimators`), and the maximum depth of the trees (`max_depth`) as inputs, and outputs the trained model. The command runs in a pre-configured AzureML environment with all the necessary libraries. The job produces a trained **Random Forest Regressor model**, which can be used for predicting the price of used cars based on the given attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # ------- WRITE YOUR CODE HERE -------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2.3 Registering the Best Trained Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The **Model Registration job** is designed to take the best-trained model from the hyperparameter tuning sweep job and register it in MLflow as a versioned artifact for future use in the used car price prediction pipeline. This job script accepts one input: the path to the trained model (model). The script begins by loading the model using the `mlflow.sklearn.load_model()` function. Afterward, it registers the model in the MLflow model registry, assigning it a descriptive name (`used_cars_price_prediction_model`) and specifying an artifact path (`random_forest_price_regressor`) where the model artifacts will be stored. Using MLflow's `log_model()` function, the model is logged along with its metadata, ensuring that the model is easily trackable and retrievable for future evaluation, deployment, or retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command, Input, Output\n",
        "\n",
        "step_process = command(\n",
        "    name=\"data_preparation\",\n",
        "    display_name=\"Data Preparation for Automated Vehicle Pricing\",\n",
        "    description=\"Prepare and split data into train and test sets\",\n",
        "    inputs={ \n",
        "        \"data\": Input(type=\"uri_file\"),\n",
        "        \"test_train_ratio\": Input(type=\" \"),\n",
        "    },\n",
        "    outputs={  \n",
        "        \"train_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        \"test_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    },\n",
        "    code=\"./data_prep\",\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} \\\n",
        "            --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} \\\n",
        "            --test_data ${{outputs.test_data}}\"\"\",\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Define Model Register Job**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "For this AzureML job, a `command` object is defined to execute the `model_register.py` script. It accepts the best-trained model as input, runs the script in the `AzureML-sklearn-1.0-ubuntu20.04-py38-cpu` environment, and uses the same compute cluster as the previous jobs (`cpu-cluster`). This job plays a crucial role in the pipeline by ensuring that the best-performing model identified during hyperparameter tuning is systematically stored and made available in the MLflow registry for further evaluation, deployment, or retraining. Integrating this job into the end-to-end pipeline automates the process of registering high-quality models, completing the model development lifecycle and enabling the prediction of used car prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command, Input, Output\n",
        "\n",
        "train_step = command(\n",
        "    name=\"train_price_prediction_model\",\n",
        "    display_name=\"Train Price Prediction Model\",\n",
        "    description=\"Train a Random Forest Regressor for used car price prediction\",\n",
        "    inputs={\n",
        "        \"train_data\": Input(type=\"uri_folder\"),\n",
        "        \"test_data\": Input(type=\"uri_folder\"),\n",
        "        \"n_estimators\": Input(type=\"number\", default=100),\n",
        "        \"max_depth\": Input(type=\"number\", default=10),\n",
        "    },\n",
        "    outputs={\n",
        "        \"model_output\": Output(type=\"mlflow_model\"),\n",
        "    },\n",
        "    code=\"./model_train\",\n",
        "    command=\"\"\"python model_train.py \\\n",
        "            --train_data ${{inputs.train_data}} \\\n",
        "            --test_data ${{inputs.test_data}} \\\n",
        "            --n_estimators ${{inputs.n_estimators}} \\\n",
        "            --max_depth ${{inputs.max_depth}} \\\n",
        "            --model_output ${{outputs.model_output}}\"\"\",\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
        "    compute=\"cpu-cluster\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory for the preprocessing script: \n",
        "import os\n",
        "\n",
        "src_dir_job_scripts = \"./model_register\"\n",
        "os.makedirs(src_dir_job_scripts, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ./model_register/model_register.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {src_dir_job_scripts}/model_register.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "mlflow.start_run()  # Starting the MLflow experiment run\n",
        "\n",
        "def main():\n",
        "    # Argument parser setup for command line arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, help=\"Path to the trained model\")  # Path to the trained model artifact\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load the trained model from the provided path\n",
        "    model = mlflow.sklearn.load_model(args.model)\n",
        "\n",
        "    print(\"Registering the best trained used cars price prediction model\")\n",
        "    \n",
        "    # Register the model in the MLflow Model Registry under the name \"price_prediction_model\"\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model,\n",
        "        registered_model_name=\"used_cars_price_prediction_model\",\n",
        "        artifact_path=\"random_forest_price_regressor\"\n",
        "    )\n",
        "\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command, Input\n",
        "\n",
        "model_register_component = command(\n",
        "    name=\"register_model\", \n",
        "    display_name=\"Register Best Model\",\n",
        "    description=\"Register the best trained model in MLflow Model Registry\",\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"mlflow_model\"), \n",
        "    },\n",
        "    code=\"./model_register\",\n",
        "    command=\"\"\"python  model_register.py \\\n",
        "            --model ${{inputs.model}}\"\"\",\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
        "    compute=\"cpu-cluster\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.sweep import Choice\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import ModelType\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "# Assemble the pipeline by chaining the jobs\n",
        "@pipeline(\n",
        "    compute=\"cpu-cluster\",\n",
        "    description=\"End-to-end MLOps pipeline for used car price prediction\",\n",
        ")\n",
        "def complete_pipeline(input_data_uri, test_train_ratio, n_estimators, max_depth):\n",
        "    \n",
        "    # Step 1: Preprocess the data\n",
        "    preprocess_step = step_process(\n",
        "        data=input_data_uri,\n",
        "        test_train_ratio=test_train_ratio,\n",
        "    )\n",
        "    \n",
        "    # Step 2: Train the model using preprocessed data\n",
        "    training_step = train_step(\n",
        "        train_data=preprocess_step.outputs.train_data,\n",
        "        test_data=preprocess_step.outputs.test_data,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "    )\n",
        "    \n",
        "    # Define the training step with hyperparameters for tuning\n",
        "    job_for_sweep = training_step(\n",
        "        n_estimators=Choice(values=[10, 20, 30, 50]),\n",
        "        max_depth=Choice(values=[5, 10, 15, 20]),\n",
        "    )\n",
        "\n",
        "    # Define the sweep job\n",
        "    sweep_job = job_for_sweep.sweep(\n",
        "        compute=\"cpu-cluster\",\n",
        "        sampling_algorithm=\"random\",\n",
        "        primary_metric=\"MSE\",\n",
        "        goal=\"Minimize\",\n",
        "    )\n",
        "\n",
        "    # Set the limits for the sweep job:\n",
        "    # - max_total_trials: The maximum number of hyperparameter combinations to be evaluated (20 in this case).\n",
        "    # - max_concurrent_trials: The maximum number of trials to run simultaneously (10 in this case) to optimize resource utilization.\n",
        "    # - timeout: The maximum allowed duration for the sweep job in seconds (7200 seconds, or 2 hours).\n",
        "    sweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n",
        "    \n",
        "    # Step 3: Register the best model\n",
        "    # After the sweep job, get the best model\n",
        "    model_register_step = model_register_component(\n",
        "        model=job_for_sweep.outputs.model_output,\n",
        "    )\n",
        "\n",
        "    # Returning outputs from all steps in the pipeline\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": preprocess_step.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": preprocess_step.outputs.test_data,\n",
        "        \"pipeline_job_best_model\": job_for_sweep.outputs.model_output,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2.4. Assembling the End-to-End Workflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The end-to-end pipeline integrates all the previously defined jobs into a seamless workflow, automating the process of data preparation, model training, hyperparameter tuning, and model registration. The pipeline is designed using Azure Machine Learning's `@pipeline` decorator, specifying the compute target and providing a detailed description of the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attribute errorMessage: Invalid type ValidationError for attribute value. Expected one of ['NoneType', 'bool', 'bytes', 'int', 'float', 'str', 'Sequence', 'Mapping'] or a sequence of those types\n"
          ]
        },
        {
          "ename": "MlException",
          "evalue": "Value ' ' passed is not in set ['boolean', 'integer', 'number', 'string']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/_schema/core/fields.py:630\u001b[0m, in \u001b[0;36mTypeSensitiveUnionField._serialize\u001b[0;34m(self, value, attr, obj, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTypeSensitiveUnionField\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/_schema/core/fields.py:479\u001b[0m, in \u001b[0;36mUnionField._serialize\u001b[0;34m(self, value, attr, obj, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         errors\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mstr\u001b[39m(e)])\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ValidationError(message\u001b[38;5;241m=\u001b[39merrors, field_name\u001b[38;5;241m=\u001b[39mattr)\n",
            "\u001b[0;31mValidationError\u001b[0m: [\"Value ' ' passed is not in set ['boolean', 'integer', 'number', 'string']\", \"Value ' ' passed is not in set ['path', 'custom_model', 'mlflow_model', 'mltable', 'triton_model', 'uri_file', 'uri_folder']\", 'Serialize on FileRefField is not supported.']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:677\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_validation:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;66;03m# Create all dependent resources\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:371\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[0;32m--> 371\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:579\u001b[0m, in \u001b[0;36mJobOperations._validate\u001b[0;34m(self, job, raise_on_failure, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m git_code_validation_result\u001b[38;5;241m.\u001b[39mtry_raise(\n\u001b[1;32m    575\u001b[0m         raise_error\u001b[38;5;241m=\u001b[39mraise_on_failure,\n\u001b[1;32m    576\u001b[0m         error_func\u001b[38;5;241m=\u001b[39merror_func,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[0;32m--> 579\u001b[0m validation_result \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_failure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m validation_result\u001b[38;5;241m.\u001b[39mmerge_with(git_code_validation_result)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_validation/schema.py:119\u001b[0m, in \u001b[0;36mSchemaValidatableMixin._validate\u001b[0;34m(self, raise_error)\u001b[0m\n\u001b[1;32m    118\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__schema_validate()\n\u001b[0;32m--> 119\u001b[0m result\u001b[38;5;241m.\u001b[39mmerge_with(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_customized_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_raise(result, raise_error\u001b[38;5;241m=\u001b[39mraise_error)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_job/pipeline/pipeline_job.py:294\u001b[0m, in \u001b[0;36mPipelineJob._customized_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent, PipelineComponent):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# Merge with pipeline component validate result for structure validation.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# Skip top level parameter missing type error\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     validation_result\u001b[38;5;241m.\u001b[39mmerge_with(\n\u001b[0;32m--> 294\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_customized_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    295\u001b[0m         condition_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39merror_code \u001b[38;5;241m==\u001b[39m ValidationErrorCode\u001b[38;5;241m.\u001b[39mPARAMETER_TYPE_UNKNOWN\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39myaml_path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Validate compute\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_component/pipeline_component.py:150\u001b[0m, in \u001b[0;36mPipelineComponent._customized_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BaseNode):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Node inputs will be validated.\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     validation_result\u001b[38;5;241m.\u001b[39mmerge_with(\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjobs.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(node_name))\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mcomponent, Component):\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# Validate binding if not remote resource.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_validation/schema.py:118\u001b[0m, in \u001b[0;36mSchemaValidatableMixin._validate\u001b[0;34m(self, raise_error)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate the resource. If raise_error is True, raise ValidationError if validation fails and log warnings if\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mapplicable; Else, return the validation result.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m:rtype: MutableValidationResult\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__schema_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m result\u001b[38;5;241m.\u001b[39mmerge_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_customized_validate())\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_validation/schema.py:151\u001b[0m, in \u001b[0;36mSchemaValidatableMixin.__schema_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate the resource with the schema.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m:return: The validation result\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m:rtype: MutableValidationResult\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dump_for_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_for_validation\u001b[38;5;241m.\u001b[39mvalidate(data)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_validation/path_aware_schema.py:53\u001b[0m, in \u001b[0;36mPathAwareSchemaValidatableMixin._dump_for_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_dump_for_validation\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mDict:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# this is not a necessary step but to keep the same behavior as before\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# empty items will be removed when converting to dict\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m typing\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mdict\u001b[39m, convert_ordered_dict_to_dict(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dump_for_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/entities/_validation/schema.py:84\u001b[0m, in \u001b[0;36mSchemaValidatableMixin._dump_for_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert the resource to a dictionary.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m:return: Converted dictionary\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m:rtype: typing.Dict\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m res: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema_for_validation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/marshmallow/schema.py:621\u001b[0m, in \u001b[0;36mSchema.dump\u001b[0;34m(self, obj, many)\u001b[0m\n\u001b[1;32m    619\u001b[0m     processed_obj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m--> 621\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmany\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmany\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks[POST_DUMP]:\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/marshmallow/schema.py:589\u001b[0m, in \u001b[0;36mSchema._serialize\u001b[0;34m(self, obj, many)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr_name, field_obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_fields\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 589\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfield_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m missing:\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/marshmallow/fields.py:348\u001b[0m, in \u001b[0;36mField.serialize\u001b[0;34m(self, attr, obj, accessor, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/_schema/core/fields.py:632\u001b[0m, in \u001b[0;36mTypeSensitiveUnionField._serialize\u001b[0;34m(self, value, attr, obj, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simplified_error_base_on_type(e, value, attr)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[0;31mValidationError\u001b[0m: Value ' ' passed is not in set ['boolean', 'integer', 'number', 'string']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMlException\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m pipeline_instance \u001b[38;5;241m=\u001b[39m complete_pipeline(\n\u001b[1;32m      6\u001b[0m     input_data_uri\u001b[38;5;241m=\u001b[39mInput(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39mdata_path),\n\u001b[1;32m      7\u001b[0m     test_train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      8\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Submit the pipeline to Azure ML\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice_prediction_pipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Stream the output of the job for real-time logs\u001b[39;00m\n\u001b[1;32m     19\u001b[0m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mstream(pipeline_job\u001b[38;5;241m.\u001b[39mname)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/core/tracing/decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:371\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameter_dimensions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[0;32m--> 371\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[1;32m    374\u001b[0m         activityLogger\u001b[38;5;241m.\u001b[39mactivity_info\u001b[38;5;241m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:682\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_arm_id_or_upload_dependencies(job)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationException, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 682\u001b[0m     \u001b[43mlog_and_raise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m git_props \u001b[38;5;241m=\u001b[39m get_git_properties()\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# Do not add git props if they already exist in job properties.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# This is for update specifically-- if the user switches branches and tries to update\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# their job, the request will fail since the git props will be repopulated.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# MFE does not allow existing properties to be updated, only for new props to be added\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azure/ai/ml/_exception_helper.py:337\u001b[0m, in \u001b[0;36mlog_and_raise_error\u001b[0;34m(error, debug, yaml_operation)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MlException(message\u001b[38;5;241m=\u001b[39mformatted_error, no_personal_data_message\u001b[38;5;241m=\u001b[39mformatted_error)\n",
            "\u001b[0;31mMlException\u001b[0m: Value ' ' passed is not in set ['boolean', 'integer', 'number', 'string']"
          ]
        }
      ],
      "source": [
        "# The code retrieves a specific version of a registered data asset using the ml_client object.\n",
        "data_path = ml_client.data.get(\"used-cars-data\", version=\"1\").path\n",
        "\n",
        "# Create pipeline instance\n",
        "pipeline_instance = complete_pipeline(\n",
        "    input_data_uri=Input(type=\"uri_file\", path=data_path),\n",
        "    test_train_ratio=0.2,\n",
        "    n_estimators=50,\n",
        "    max_depth=5\n",
        ")\n",
        "\n",
        "# Submit the pipeline to Azure ML\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_instance, \n",
        "    experiment_name=\"price_prediction_pipeline\"\n",
        ")\n",
        "\n",
        "# Stream the output of the job for real-time logs\n",
        "ml_client.jobs.stream(pipeline_job.name)\n",
        "\n",
        "# Access pipeline outputs (optional, after job completion)\n",
        "print(f\"Train data location: {pipeline_job.outputs['pipeline_job_train_data']}\")\n",
        "print(f\"Test data location: {pipeline_job.outputs['pipeline_job_test_data']}\")\n",
        "print(f\"Best model location: {pipeline_job.outputs['pipeline_job_best_model']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
